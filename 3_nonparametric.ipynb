{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Non-parametric Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- [Polynomial Regression and Step Functions](#Polynomial-Regression-and-Step-Functions)\n",
    "- [Splines](#Splines)\n",
    "- [Local Regression](#Local-Regression)\n",
    "- [Generalized-Additive-Models](#Generalized-Additive-Models)\n",
    "\n",
    "## Lab\n",
    "\n",
    "- [Non-linear Modeling](#7.8-Non-linear-Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# Set global parameters\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7c734dc8918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Wage.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Wage.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression and Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create polynomials for 'age'. These correspond to those in R, when using raw=TRUE in poly() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))\n",
    "X2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))\n",
    "X3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))\n",
    "X4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))\n",
    "X5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))\n",
    "\n",
    "y = (df.wage > 250).map({False:0, True:1}).values\n",
    "print('X4:\\n', X4[:5])\n",
    "print('y:\\n', y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model. (Degree 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 = sm.GLS(df.wage, X4).fit()\n",
    "fit2.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a suitable degree for the polynomial of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_1 =  sm.GLS(df.wage, X1).fit()\n",
    "fit_2 =  sm.GLS(df.wage, X2).fit()\n",
    "fit_3 =  sm.GLS(df.wage, X3).fit()\n",
    "fit_4 =  sm.GLS(df.wage, X4).fit()\n",
    "fit_5 =  sm.GLS(df.wage, X5).fit()\n",
    "\n",
    "sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial degree 4 seems best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implements a regularized logistic regression model particularly suitable for high dimensional data. Since we just have one feature (age) we use the GLM model from statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "res = clf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array of test data. Transform to polynomial degree 4 and run prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = PolynomialFeatures(4).fit_transform(age_grid)\n",
    "pred = res.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating plots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "fig.suptitle('Degree-4 Polynomial', fontsize=18)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3)\n",
    "sns.regplot(x=df.age, y=df.wage, order = 4, truncate=True, scatter=False, ax=ax1)\n",
    "ax1.set_ylim(ymin=0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(age_grid, pred, color='b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s=30, c='grey', marker='|', alpha=0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01,0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut, bins = pd.cut(df.age, 4, retbins=True, right=True)\n",
    "df_cut.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps = pd.concat([df.age, df_cut, df.wage], keys=['age','age_cuts','wage'], axis=1)\n",
    "df_steps.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the age groups\n",
    "df_steps_dummies = pd.get_dummies(df_steps['age_cuts'])\n",
    "\n",
    "# Statsmodels requires explicit adding of a constant (intercept)\n",
    "df_steps_dummies = sm.add_constant(df_steps_dummies)\n",
    "\n",
    "df_steps_dummies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using statsmodels because it has a more complete output for coefficients\n",
    "fit3 = sm.GLM(df_steps.wage, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1)).fit()\n",
    "fit3.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the test data in the same bins as the training data.\n",
    "bin_mapping = np.digitize(age_grid.ravel(), bins)\n",
    "bin_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies, drop first dummy category, add constant\n",
    "X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis=1))\n",
    "X_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = fit3.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = sm.GLM(y, df_steps_dummies.drop(df_steps_dummies.columns[1], axis=1),\n",
    "              family=sm.families.Binomial(sm.families.links.logit))\n",
    "res2 = clf2.fit()\n",
    "pred3 = res2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating plots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "fig.suptitle('Piecewise Constant', fontsize=14)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3)\n",
    "ax1.plot(age_grid, pred2, c='b')\n",
    "\n",
    "ax1.set_xlabel('age')\n",
    "ax1.set_ylabel('wage')\n",
    "ax1.set_ylim(ymin=0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(np.arange(df.age.min(), df.age.max()).reshape(-1,1), pred3, color='b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s=30, c='grey', marker='|', alpha=0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01,0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using patsy to create non-linear transformations of the input data. See http://patsy.readthedocs.org/en/latest/ <BR>\n",
    "I have not found functions to create smoothing splines or GAMs or do local regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubic splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying 3 knots\n",
    "transformed_x = dmatrix(\"bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit4 = sm.GLM(df.wage, transformed_x).fit()\n",
    "pred4 = fit4.predict(dmatrix(\"bs(age_grid, knots=(25,40,60), degree=3, include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "fit4.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying 6 degrees of freedom \n",
    "transformed_x2 = dmatrix(\"bs(df.age, df=6, degree=3, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit5 = sm.GLM(df.wage, transformed_x2).fit()\n",
    "pred5 = fit5.predict(dmatrix(\"bs(age_grid, df=6, degree=3, include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "fit5.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying 4 degrees of freedom\n",
    "transformed_x3 = dmatrix(\"cr(df.age, df=4)\", {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit6 = sm.GLM(df.wage, transformed_x3).fit()\n",
    "pred6 = fit6.predict(dmatrix(\"cr(age_grid, df=4)\", {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "fit6.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3)\n",
    "plt.plot(age_grid, pred4, color='b', label='Specifying three knots')\n",
    "plt.plot(age_grid, pred5, color='r', label='Specifying df=6')\n",
    "plt.plot(age_grid, pred6, color='g', label='Natural spline df=4')\n",
    "[plt.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='b') for i in [25,40,60]]\n",
    "plt.legend(bbox_to_anchor=(1.5, 1.0))\n",
    "plt.xlim(15,85)\n",
    "plt.ylim(0,350)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('wage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does local regression work?\n",
    "\n",
    "Ingredients: $X$, $y$.\n",
    "\n",
    "How to you output a prediction $\\hat y_i$ at a new point $x_i$?\n",
    "\n",
    "1. Take a number of points in $X$ close to $x_i$: $X_{\\text{close-to-i}}$\n",
    "2. Assign a weight to each of there points\n",
    "3. Fit a weigthed least squares regression of $X_{\\text{close-to-i}}$ on $y_{\\text{close-to-i}}$\n",
    "4. Use the estimated coefficients $\\hat \\beta$ to predict $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generate data\n",
    "X_sim = np.sort(np.random.uniform(0,1,100))\n",
    "e = np.random.uniform(-.5,.5,100)\n",
    "y_sim = -4*X_sim**2 + 3*X_sim + e\n",
    "\n",
    "# True Generating process without noise\n",
    "X_grid = np.linspace(0,1,100)\n",
    "y_grid = -4*X_grid**2 + 3*X_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit local linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "\n",
    "# Settings\n",
    "spec = 'll'\n",
    "bandwidth = 0.1\n",
    "kernel = 'gaussian'\n",
    "\n",
    "# Locally linear regression\n",
    "local_reg = KernelReg(y_sim, X_sim.reshape(-1,1), var_type='c', reg_type=spec, bw=[bandwidth], ckertype=kernel)\n",
    "y_hat = KernelReg.fit(local_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the parameters mean?\n",
    " - `var_type`: the dependent variable is continuous\n",
    " - `reg_type`: the local regressio is linear\n",
    " - `bw`: bandwidth length\n",
    " - `ckertype`: kernel type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \n",
    "x_i = 0.5\n",
    "close_to_i = (x_i-bandwidth < X_sim) & (X_sim < x_i+bandwidth)\n",
    "X_tilde = X_sim[close_to_i]\n",
    "y_tilde = y_sim[close_to_i]\n",
    "\n",
    "# Get local estimates\n",
    "local_estimate = KernelReg.fit(local_reg, data_predict=[x_i])\n",
    "y_i_hat = local_estimate[0]\n",
    "beta_i_hat = local_estimate[1]\n",
    "alpha_i_hat = y_i_hat - beta_i_hat*x_i\n",
    "print('Estimates: alpha=%1.4f, beta=%1.4f' % (alpha_i_hat, beta_i_hat))\n",
    "\n",
    "# Build local predictions\n",
    "close_to_i_grid = (x_i-bandwidth < X_grid) & (X_grid < x_i+bandwidth)\n",
    "X_grid_tilde = X_grid[close_to_i_grid].reshape(-1,1)\n",
    "y_grid_tilde = alpha_i_hat + X_grid_tilde*beta_i_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now trying to predict the value of $\\hat y_i(x_i)$ given that $x_i = 0.3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Figure 7.9\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Figure 7.9', fontsize=20);\n",
    "\n",
    "# Plot scatter and lines\n",
    "ax.scatter(X_sim, y_sim, facecolor='None', edgecolor='k', alpha=0.3);\n",
    "ax.plot(X_grid, y_grid);\n",
    "ax.plot(X_sim, y_hat[0]);\n",
    "\n",
    "# Add local details\n",
    "ax.scatter(X_tilde, y_tilde, facecolor='orange', edgecolor='None', alpha=0.5);\n",
    "ax.scatter(x_i, y_i_hat, facecolor='r', alpha=1);\n",
    "ax.plot(X_grid_tilde, y_grid_tilde, color='r');\n",
    "\n",
    "# Legend\n",
    "ax.legend(['True relationship', 'LLN Estimate', 'Local OLS']);\n",
    "ax.annotate(\"$x_i$\", (x_i, y_i_hat-0.2), color='r', fontsize=20);\n",
    "ax.set_xlabel('X'); ax.set_ylabel('y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we zoom in? We can do the local regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Local OLS', fontsize=20);\n",
    "\n",
    "# Zoom in\n",
    "sns.regplot(x=X_tilde, y=y_tilde, ax=ax, order=1, \n",
    "            ci=None, scatter=False, line_kws={'color':'red'})\n",
    "ax.scatter(X_tilde,y_tilde, facecolor='orange', edgecolor='None', alpha=0.5, s=80);\n",
    "ax.scatter(x_i, y_i_hat, facecolor='r', alpha=1);\n",
    "ax.annotate(\"$x_i$\", (x_i, y_i_hat-0.1), color='r', fontsize=20);\n",
    "ax.set_xlabel('local X'); ax.set_ylabel('local y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the line upward sloped? We forgot the gaussian weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Weights\n",
    "w = norm.pdf((X_sim-x_i)/bandwidth)\n",
    "\n",
    "# Estimate LWS\n",
    "mod_wls = sm.WLS(y_sim, sm.add_constant(X_sim), weights=w)\n",
    "results = mod_wls.fit()\n",
    "\n",
    "print('Estimates: alpha=%1.4f, beta=%1.4f' % tuple(results.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed got the same estimates as before. Note two things:\n",
    "1. the badwidth defines the scale parameter of the gaussian weights\n",
    "2. our locally linear regression is acqually global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Local WLS', fontsize=20);\n",
    "\n",
    "# Zoom in\n",
    "points = ax.scatter(X_sim, y_sim, c=w, cmap=\"YlOrRd\", edgecolors='None', alpha=.5, s=80);\n",
    "plt.colorbar(points, label='gaussian weights')\n",
    "ax.plot(X_grid, results.params[0] + results.params[1]*X_grid, color='r')\n",
    "ax.scatter(x_i, y_i_hat, facecolor='r', alpha=1);\n",
    "ax.annotate(\"$x_i$\", (x_i, y_i_hat-0.1), color='r', fontsize=20);\n",
    "ax.set_xlabel('local X'); ax.set_ylabel('local y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Additive Models (GAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine to extend the general regression framework to some separabily additive model of the form\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\sum_{k=1}^K \\beta_k f(x_{ik}) + \\varepsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pygam import LinearGAM, s, f\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df['education_'] = LabelEncoder().fit_transform(df[\"education\"])\n",
    "X = df[['year','age','education_']].to_numpy()\n",
    "y = df[['wage']].to_numpy()\n",
    "\n",
    "## model\n",
    "gam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2))\n",
    "gam.gridsearch(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 7.13\n",
    "fig, axs = plt.subplots(1,3,figsize=(15,5));\n",
    "\n",
    "# Plot\n",
    "titles = ['year', 'age', 'education']\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    pdep, confi = gam.partial_dependence(term=i, width=.95)\n",
    "    ax.plot(XX[:, i], pdep)\n",
    "    ax.plot(XX[:, i], confi, c='k', ls=':', alpha=0.5)\n",
    "    ax.set_title(titles[i]);\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-40,40)\n",
    "    ax.set_title(titles[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LogisticGAM\n",
    "\n",
    "# Binary dependent variable\n",
    "y_binary = (y>250)\n",
    "\n",
    "## Logit link function\n",
    "logit_gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=10) + f(2), fit_intercept=True)\n",
    "logit_gam.gridsearch(X, y_binary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Figure 7.13\n",
    "fig, axs = plt.subplots(1,3,figsize=(15,5));\n",
    "\n",
    "# Plot\n",
    "titles = ['year', 'age', 'education']\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = logit_gam.generate_X_grid(term=i)\n",
    "    pdep, confi = logit_gam.partial_dependence(term=i, width=.95)\n",
    "    ax.plot(XX[:, i], pdep, c='g')\n",
    "    ax.plot(XX[:, i], confi, c='k', ls=':', alpha=0.5)\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-4,4)\n",
    "    ax.set_title(titles[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Non-linear Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll explore how to generate the `Wage` dataset models we saw in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Wage.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.1 Polynomial Regression and Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit the polynomial regression model using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))\n",
    "X2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))\n",
    "X3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))\n",
    "X4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))\n",
    "X5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This syntax fits a linear model, using the `PolynomialFeatures()` function, in order to predict\n",
    "wage using up to a fourth-degree polynomial in `age`. The `PolynomialFeatures()` command\n",
    "allows us to avoid having to write out a long formula with powers\n",
    "of `age`. We can then fit our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 = sm.GLS(df.wage, X4).fit()\n",
    "fit2.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we consider the task of predicting whether an individual earns more\n",
    "than \\$250,000 per year. We proceed much as before, except that first we\n",
    "create the appropriate response vector, and then we fit a logistic model using the `GLM()` function from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response matrix\n",
    "y = (df.wage > 250).map({False:0, True:1}).values\n",
    "\n",
    "# Fit logistic model\n",
    "clf = sm.GLM(y, X4, family=sm.families.Binomial(sm.families.links.logit))\n",
    "res = clf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a grid of values for `age` at which we want predictions, and\n",
    "then call the generic `predict()` function for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)\n",
    "\n",
    "# Generate test data\n",
    "X_test = PolynomialFeatures(4).fit_transform(age_grid)\n",
    "\n",
    "# Predict the value of the generated ages\n",
    "pred1 = fit2.predict(X_test) # salary\n",
    "pred2 = res.predict(X_test)  # Pr(wage>250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the data and add the fit from the degree-4 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating plots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (16,5))\n",
    "fig.suptitle('Degree-4 Polynomial', fontsize=14)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3)\n",
    "ax1.plot(age_grid, pred1, color = 'b')\n",
    "ax1.set_ylim(ymin=0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(age_grid, pred2, color='b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s=30, c='grey', marker='|', alpha=0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01,0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding on a degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In performing a polynomial regression we must decide on the degree of\n",
    "the polynomial to use. One way to do this is by using hypothesis tests. We\n",
    "now fit models ranging from linear to a degree-5 polynomial and seek to\n",
    "determine the simplest model which is sufficient to explain the relationship\n",
    "between `wage` and `age`.\n",
    "\n",
    "We can do this using the `anova_lm()` function, which performs an\n",
    "analysis of variance (ANOVA, using an F-test) in order to test the null\n",
    "hypothesis that a model $M_1$ is sufficient to explain the data against the \n",
    "alternative hypothesis that a more complex model $M_2$ is required. In order\n",
    "to use the `anova_lm()` function, $M_1$ and $M_2$ must be **nested models**: the\n",
    "predictors in $M_1$ must be a subset of the predictors in $M_2$. In this case,\n",
    "we fit five different models and sequentially compare the simpler model to\n",
    "the more complex model (*Note:* you may get an *invalid value* Runtime Warning on the first model, because there is no \"simpler model\" to compare to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_1 = fit = sm.GLS(df.wage, X1).fit()\n",
    "fit_2 = fit = sm.GLS(df.wage, X2).fit()\n",
    "fit_3 = fit = sm.GLS(df.wage, X3).fit()\n",
    "fit_4 = fit = sm.GLS(df.wage, X4).fit()\n",
    "fit_5 = fit = sm.GLS(df.wage, X5).fit()\n",
    "\n",
    "print(sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-value comparing the linear Model 1 to the quadratic Model 2 is\n",
    "essentially zero $(<10^{-32})$, indicating that a linear fit is not sufficient. Similarly\n",
    "the $p$-value comparing the quadratic Model 2 to the cubic Model 3\n",
    "is very low (0.0017), so the quadratic fit is also insufficient. The $p$-value\n",
    "comparing the cubic and degree-4 polynomials, Model 3 and Model 4, is approximately\n",
    "0.05 while the degree-5 polynomial Model 5 seems unnecessary\n",
    "because its $p$-value is 0.37. Hence, either a cubic or a quartic polynomial\n",
    "appear to provide a reasonable fit to the data, but lower- or higher-order\n",
    "models are not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to using hypothesis tests and ANOVA, we could choose\n",
    "the polynomial degree using cross-validation as we have in previous labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a step function, we use the `cut()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut, bins = pd.cut(df.age, 4, retbins = True, right = True)\n",
    "df_cut.value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `cut()` automatically picked the cutpoints at 33.5, 49, and 64.5 years\n",
    "of age. We could also have specified our own cutpoints directly. Now let's create a set of dummy variables for use in the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps = pd.concat([df.age, df_cut, df.wage], keys = ['age','age_cuts','wage'], axis = 1)\n",
    "\n",
    "# Create dummy variables for the age groups\n",
    "df_steps_dummies = pd.get_dummies(df_steps['age_cuts'])\n",
    "\n",
    "# Statsmodels requires explicit adding of a constant (intercept)\n",
    "df_steps_dummies = sm.add_constant(df_steps_dummies)\n",
    "\n",
    "# Drop the (17.938, 33.5] category\n",
    "df_steps_dummies = df_steps_dummies.drop(df_steps_dummies.columns[1], axis = 1)\n",
    "\n",
    "df_steps_dummies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An now to fit the models! We dropped the `age<33.5` category, so the intercept coefficient of\n",
    "\\$94,160 can be interpreted as the average salary for those under 33.5 years\n",
    "of age. The other coefficients can be interpreted as the average additional\n",
    "salary for those in the other age groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit3 = sm.GLM(df_steps.wage, df_steps_dummies).fit()\n",
    "fit3.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can produce predictions\n",
    "and plots just as we did in the case of the polynomial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the test data in the same bins as the training data.\n",
    "bin_mapping = np.digitize(age_grid.ravel(), bins)\n",
    "\n",
    "# Get dummies, drop first dummy category, add constant\n",
    "X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis = 1))\n",
    "\n",
    "# Predict the value of the generated ages using the linear model\n",
    "pred2 = fit3.predict(X_test2)\n",
    "\n",
    "# And the logistic model\n",
    "clf2 = sm.GLM(y, df_steps_dummies,\n",
    "              family=sm.families.Binomial(sm.families.links.logit))\n",
    "res2 = clf2.fit()\n",
    "pred3 = res2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,5))\n",
    "fig.suptitle('Piecewise Constant', fontsize = 14)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor = 'None', edgecolor = 'k', alpha = 0.3)\n",
    "ax1.plot(age_grid, pred2, c = 'b')\n",
    "\n",
    "ax1.set_xlabel('age')\n",
    "ax1.set_ylabel('wage')\n",
    "ax1.set_ylim(ymin = 0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(np.arange(df.age.min(), df.age.max()).reshape(-1,1), pred3, color = 'b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s = 30, c = 'grey', marker = '|', alpha = 0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01, 0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.2 Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit regression splines in python, we use the ${\\tt dmatrix}$ module from the ${\\tt patsy}$ library. In lecture, we saw that regression splines can be fit by constructing an appropriate matrix of basis functions. The ${\\tt bs()}$ function generates the entire matrix of basis functions for splines with the specified set of knots.  Fitting ${\\tt wage}$ to ${\\tt age}$ using a regression spline is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "# Specifying 3 knots\n",
    "transformed_x1 = dmatrix(\"bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "\n",
    "# Build a regular linear model from the splines\n",
    "fit1 = sm.GLM(df.wage, transformed_x1).fit()\n",
    "fit1.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have prespecified knots at ages 25, 40, and 60. This produces a\n",
    "spline with six basis functions. (Recall that a cubic spline with three knots\n",
    "has seven degrees of freedom; these degrees of freedom are used up by an\n",
    "intercept, plus six basis functions.) We could also use the ${\\tt df}$ option to\n",
    "produce a spline with knots at uniform quantiles of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying 6 degrees of freedom \n",
    "transformed_x2 = dmatrix(\"bs(df.age, df=6, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit2 = sm.GLM(df.wage, transformed_x2).fit()\n",
    "fit2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case python chooses knots which correspond\n",
    "to the 25th, 50th, and 75th percentiles of ${\\tt age}$. The function ${\\tt bs()}$ also has\n",
    "a ${\\tt degree}$ argument, so we can fit splines of any degree, rather than the\n",
    "default degree of 3 (which yields a cubic spline).\n",
    "\n",
    "In order to instead fit a natural spline, we use the ${\\tt cr()}$ function. Here\n",
    "we fit a natural spline with four degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying 4 degrees of freedom\n",
    "transformed_x3 = dmatrix(\"cr(df.age, df=4)\", {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit3 = sm.GLM(df.wage, transformed_x3).fit()\n",
    "fit3.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the ${\\tt bs()}$ function, we could instead specify the knots directly using\n",
    "the ${\\tt knots}$ option.\n",
    "\n",
    "Let's see how these three models stack up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)\n",
    "\n",
    "# Make some predictions\n",
    "pred1 = fit1.predict(dmatrix(\"bs(age_grid, knots=(25,40,60), include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "pred2 = fit2.predict(dmatrix(\"bs(age_grid, df=6, include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "pred3 = fit3.predict(dmatrix(\"cr(age_grid, df=4)\", {\"age_grid\": age_grid}, return_type='dataframe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the splines and error bands\n",
    "plt.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.1)\n",
    "plt.plot(age_grid, pred1, color='b', label='Specifying three knots')\n",
    "plt.plot(age_grid, pred2, color='r', label='Specifying df=6')\n",
    "plt.plot(age_grid, pred3, color='g', label='Natural spline df=4')\n",
    "[plt.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='b') for i in [25,40,60]]\n",
    "plt.legend()\n",
    "plt.xlim(15,85)\n",
    "plt.ylim(0,350)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to [Session 4 - Complexity and Regularization](https://nbviewer.jupyter.org/github/matteocourthoud/Machine-Learning-for-Economic-Analysis-2020/blob/master/4_regularization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
